{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Vision Transformer for Speech Enhancement - Complete Demo\n",
    "\n",
    "**Just click 'Run All' and wait for the complete demonstration!**\n",
    "\n",
    "This notebook will:\n",
    "1. Install all dependencies\n",
    "2. Download the dataset automatically\n",
    "3. Define the complete model architecture\n",
    "4. Train the model\n",
    "5. Evaluate performance\n",
    "6. Demonstrate audio enhancement\n",
    "\n",
    "**Estimated time**: 30-60 minutes (depending on GPU availability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install required packages\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "!pip install librosa soundfile matplotlib seaborn tqdm einops\n",
    "!pip install kagglehub\n",
    "\n",
    "print(\"✓ All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from IPython.display import Audio, display\n",
    "from tqdm.notebook import tqdm\n",
    "from einops import rearrange\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Check device\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Device: {device}\")\n",
    "if device == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# Set random seeds\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if device == 'cuda':\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "print(\"\\n✓ Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Download Dataset Automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "import shutil\n",
    "\n",
    "data_root = Path('demo_data')\n",
    "data_root.mkdir(exist_ok=True)\n",
    "\n",
    "try:\n",
    "    print(\"Attempting to download dataset from Kaggle...\")\n",
    "    dataset_options = [\n",
    "        \"saurabhshahane/valentini-noisy-speech-database\",\n",
    "        \"muhammadtayyab007/speech-enhancement-dataset\",\n",
    "    ]\n",
    "    \n",
    "    dataset_path = None\n",
    "    for dataset_name in dataset_options:\n",
    "        try:\n",
    "            print(f\"  Trying {dataset_name}...\")\n",
    "            dataset_path = kagglehub.dataset_download(dataset_name)\n",
    "            print(f\"  ✓ Downloaded to: {dataset_path}\")\n",
    "            break\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if dataset_path:\n",
    "        dataset_path = Path(dataset_path)\n",
    "        audio_files = list(dataset_path.rglob('*.wav'))\n",
    "        \n",
    "        if len(audio_files) > 0:\n",
    "            print(f\"\\n✓ Found {len(audio_files)} audio files!\")\n",
    "            \n",
    "            noisy_dir = data_root / 'noisy'\n",
    "            clean_dir = data_root / 'clean'\n",
    "            noisy_dir.mkdir(exist_ok=True)\n",
    "            clean_dir.mkdir(exist_ok=True)\n",
    "            \n",
    "            noisy_files = [f for f in audio_files if 'noisy' in str(f).lower()]\n",
    "            clean_files = [f for f in audio_files if 'clean' in str(f).lower()]\n",
    "            \n",
    "            max_files = min(50, len(noisy_files), len(clean_files))\n",
    "            \n",
    "            for i in range(max_files):\n",
    "                shutil.copy(noisy_files[i], noisy_dir / f'sample_{i:03d}.wav')\n",
    "                shutil.copy(clean_files[i], clean_dir / f'sample_{i:03d}.wav')\n",
    "            \n",
    "            print(f\"✓ Prepared {max_files} audio pairs\")\n",
    "        else:\n",
    "            raise Exception(\"No audio files found\")\n",
    "    else:\n",
    "        raise Exception(\"Could not download\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\nKaggle download failed: {e}\")\n",
    "    print(\"Creating synthetic dataset...\\n\")\n",
    "    \n",
    "    noisy_dir = data_root / 'noisy'\n",
    "    clean_dir = data_root / 'clean'\n",
    "    noisy_dir.mkdir(exist_ok=True)\n",
    "    clean_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    sr = 16000\n",
    "    duration = 2.0\n",
    "    num_samples = 30\n",
    "    \n",
    "    print(f\"Generating {num_samples} synthetic samples...\")\n",
    "    for i in tqdm(range(num_samples), desc=\"Creating samples\"):\n",
    "        t = np.linspace(0, duration, int(sr * duration))\n",
    "        fundamental = 200 + np.random.randn() * 50\n",
    "        \n",
    "        clean = np.zeros_like(t)\n",
    "        for harmonic in range(1, 8):\n",
    "            amplitude = 1.0 / harmonic\n",
    "            phase = np.random.rand() * 2 * np.pi\n",
    "            clean += amplitude * np.sin(2 * np.pi * fundamental * harmonic * t + phase)\n",
    "        \n",
    "        envelope = 0.5 + 0.5 * np.sin(2 * np.pi * 5 * t)\n",
    "        clean = clean * envelope\n",
    "        clean = clean / np.abs(clean).max() * 0.8\n",
    "        \n",
    "        noise = np.random.randn(len(clean)) * 0.15\n",
    "        noisy = clean + noise\n",
    "        \n",
    "        sf.write(clean_dir / f'sample_{i:03d}.wav', clean, sr)\n",
    "        sf.write(noisy_dir / f'sample_{i:03d}.wav', noisy, sr)\n",
    "    \n",
    "    print(f\"\\n✓ Created {num_samples} synthetic audio pairs\")\n",
    "\n",
    "noisy_files = sorted(list((data_root / 'noisy').glob('*.wav')))\n",
    "clean_files = sorted(list((data_root / 'clean').glob('*.wav')))\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Dataset Summary:\")\n",
    "print(f\"  Noisy files: {len(noisy_files)}\")\n",
    "print(f\"  Clean files: {len(clean_files)}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Define Hybrid Vision Transformer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Model Components\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, embed_dim, num_heads, dropout=0.1):\n        super().__init__()\n        self.embed_dim = embed_dim\n        self.num_heads = num_heads\n        self.head_dim = embed_dim // num_heads\n        self.scale = self.head_dim ** -0.5\n        \n        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n        self.proj = nn.Linear(embed_dim, embed_dim)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x):\n        B, N, C = x.shape\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n        \n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.dropout(attn)\n        \n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.dropout(x)\n        return x\n\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim, hidden_dim, dropout=0.1):\n        super().__init__()\n        self.net = nn.Sequential(\n            nn.Linear(dim, hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.Linear(hidden_dim, dim),\n            nn.Dropout(dropout)\n        )\n    \n    def forward(self, x):\n        return self.net(x)\n\n\nclass TransformerBlock(nn.Module):\n    def __init__(self, embed_dim, num_heads, mlp_ratio=4.0, dropout=0.1):\n        super().__init__()\n        self.norm1 = nn.LayerNorm(embed_dim)\n        self.attn = MultiHeadAttention(embed_dim, num_heads, dropout)\n        self.norm2 = nn.LayerNorm(embed_dim)\n        self.mlp = FeedForward(embed_dim, int(embed_dim * mlp_ratio), dropout)\n    \n    def forward(self, x):\n        x = x + self.attn(self.norm1(x))\n        x = x + self.mlp(self.norm2(x))\n        return x\n\n\nclass HybridViT(nn.Module):\n    \"\"\"Hybrid Vision Transformer for Speech Enhancement\"\"\"\n    \n    def __init__(\n        self,\n        input_channels=1,\n        output_channels=1,\n        encoder_channels=[32, 64, 128],\n        embed_dim=256,\n        num_heads=4,\n        num_layers=3,\n        mlp_ratio=4.0,\n        patch_size=4,\n        dropout=0.1,\n    ):\n        super().__init__()\n        \n        # CNN Encoder\n        self.encoder = nn.ModuleList()\n        in_ch = input_channels\n        for out_ch in encoder_channels:\n            self.encoder.append(nn.Sequential(\n                nn.Conv2d(in_ch, out_ch, 3, padding=1),\n                nn.BatchNorm2d(out_ch),\n                nn.ReLU(inplace=True),\n                nn.MaxPool2d(2) if out_ch != encoder_channels[-1] else nn.Identity()\n            ))\n            in_ch = out_ch\n        \n        # Patch Embedding\n        self.patch_embed = nn.Conv2d(encoder_channels[-1], embed_dim, patch_size, patch_size)\n        \n        # Positional Encoding\n        self.pos_embed = nn.Parameter(torch.zeros(1, 1000, embed_dim))\n        \n        # Transformer\n        self.transformer = nn.ModuleList([\n            TransformerBlock(embed_dim, num_heads, mlp_ratio, dropout)\n            for _ in range(num_layers)\n        ])\n        \n        self.norm = nn.LayerNorm(embed_dim)\n        \n        # Projection back\n        self.to_features = nn.Linear(embed_dim, encoder_channels[-1])\n        \n        # CNN Decoder\n        self.decoder = nn.ModuleList([\n            nn.Sequential(\n                nn.Upsample(scale_factor=2, mode='nearest'),\n                nn.Conv2d(128, 64, 3, padding=1),\n                nn.BatchNorm2d(64),\n                nn.ReLU(inplace=True)\n            ),\n            nn.Sequential(\n                nn.Upsample(scale_factor=2, mode='nearest'),\n                nn.Conv2d(64, 32, 3, padding=1),\n                nn.BatchNorm2d(32),\n                nn.ReLU(inplace=True)\n            ),\n            nn.Sequential(\n                nn.Conv2d(32, output_channels, 3, padding=1),\n                nn.Sigmoid()\n            )\n        ])\n    \n    def forward(self, x):\n        # Save input shape\n        input_shape = x.shape[2:]\n        \n        # Encoder\n        for block in self.encoder:\n            x = block(x)\n        \n        # Patch Embedding\n        x = self.patch_embed(x)  # [B, embed_dim, H', W']\n        B, C, H, W = x.shape\n        x = x.flatten(2).transpose(1, 2)  # [B, N, embed_dim]\n        \n        # Add positional encoding\n        x = x + self.pos_embed[:, :x.size(1), :]\n        \n        # Transformer\n        for block in self.transformer:\n            x = block(x)\n        \n        x = self.norm(x)\n        \n        # Project back\n        x = self.to_features(x)\n        x = x.transpose(1, 2).reshape(B, -1, H, W)\n        \n        # Decoder\n        for block in self.decoder:\n            x = block(x)\n        \n        # Resize to input shape\n        if x.shape[2:] != input_shape:\n            x = F.interpolate(x, size=input_shape, mode='bilinear', align_corners=False)\n        \n        return x\n\nprint(\"✓ Model architecture defined!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from torch.utils.data import Dataset, DataLoader, random_split\n\nclass AudioDataset(Dataset):\n    def __init__(self, noisy_dir, clean_dir, sr=16000, n_fft=512, hop_length=128):\n        self.noisy_files = sorted(list(Path(noisy_dir).glob('*.wav')))\n        self.clean_files = sorted(list(Path(clean_dir).glob('*.wav')))\n        self.sr = sr\n        self.n_fft = n_fft\n        self.hop_length = hop_length\n    \n    def __len__(self):\n        return len(self.noisy_files)\n    \n    def __getitem__(self, idx):\n        noisy, _ = librosa.load(self.noisy_files[idx], sr=self.sr)\n        clean, _ = librosa.load(self.clean_files[idx], sr=self.sr)\n        \n        min_len = min(len(noisy), len(clean))\n        noisy = noisy[:min_len]\n        clean = clean[:min_len]\n        \n        noisy_stft = librosa.stft(noisy, n_fft=self.n_fft, hop_length=self.hop_length)\n        clean_stft = librosa.stft(clean, n_fft=self.n_fft, hop_length=self.hop_length)\n        \n        noisy_mag = np.abs(noisy_stft)\n        clean_mag = np.abs(clean_stft)\n        \n        # Use log-scale normalization for better audio processing\n        noisy_mag_db = librosa.amplitude_to_db(noisy_mag, ref=np.max)\n        clean_mag_db = librosa.amplitude_to_db(clean_mag, ref=np.max)\n        \n        # Normalize to [0, 1] range\n        noisy_mag_norm = (noisy_mag_db + 80) / 80  # Assumes -80dB to 0dB range\n        clean_mag_norm = (clean_mag_db + 80) / 80\n        \n        noisy_mag_norm = np.clip(noisy_mag_norm, 0, 1)\n        clean_mag_norm = np.clip(clean_mag_norm, 0, 1)\n        \n        noisy_mag_norm = torch.from_numpy(noisy_mag_norm).float().unsqueeze(0)\n        clean_mag_norm = torch.from_numpy(clean_mag_norm).float().unsqueeze(0)\n        \n        return noisy_mag_norm, clean_mag_norm\n\ndataset = AudioDataset(data_root / 'noisy', data_root / 'clean')\ntrain_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n\ntrain_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, num_workers=0)\nval_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, num_workers=0)\n\nprint(f\"✓ Dataset: {len(train_dataset)} train, {len(val_dataset)} val\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Visualize Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_noisy, sample_clean = dataset[0]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "im1 = axes[0].imshow(librosa.amplitude_to_db(sample_noisy.squeeze()), aspect='auto', origin='lower', cmap='viridis')\n",
    "axes[0].set_title('Noisy Spectrogram', fontweight='bold')\n",
    "plt.colorbar(im1, ax=axes[0])\n",
    "\n",
    "im2 = axes[1].imshow(librosa.amplitude_to_db(sample_clean.squeeze()), aspect='auto', origin='lower', cmap='viridis')\n",
    "axes[1].set_title('Clean Spectrogram', fontweight='bold')\n",
    "plt.colorbar(im2, ax=axes[1])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HybridViT(\n",
    "    input_channels=1,\n",
    "    output_channels=1,\n",
    "    encoder_channels=[32, 64, 128],\n",
    "    embed_dim=256,\n",
    "    num_heads=4,\n",
    "    num_layers=3,\n",
    "    mlp_ratio=4.0,\n",
    "    patch_size=4,\n",
    "    dropout=0.1,\n",
    ").to(device)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Model Parameters: {total_params:,}\")\n",
    "\n",
    "# Test\n",
    "with torch.no_grad():\n",
    "    test_input = torch.randn(1, 1, 257, 100).to(device)\n",
    "    test_output = model(test_input)\n",
    "    print(f\"Test: {test_input.shape} → {test_output.shape}\")\n",
    "    print(\"✓ Model ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "num_epochs = 50\ncriterion = nn.L1Loss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n\ntrain_losses = []\nval_losses = []\nbest_val_loss = float('inf')\n\nprint(f\"Starting training for {num_epochs} epochs...\\n\")\n\nfor epoch in range(num_epochs):\n    # Train\n    model.train()\n    train_loss = 0.0\n    \n    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=False)\n    for noisy, clean in pbar:\n        noisy, clean = noisy.to(device), clean.to(device)\n        optimizer.zero_grad()\n        output = model(noisy)\n        loss = criterion(output, clean)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n        train_loss += loss.item()\n        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n    \n    train_loss /= len(train_loader)\n    train_losses.append(train_loss)\n    \n    # Validate\n    model.eval()\n    val_loss = 0.0\n    with torch.no_grad():\n        for noisy, clean in val_loader:\n            noisy, clean = noisy.to(device), clean.to(device)\n            output = model(noisy)\n            loss = criterion(output, clean)\n            val_loss += loss.item()\n    \n    val_loss /= len(val_loader)\n    val_losses.append(val_loss)\n    scheduler.step()\n    \n    print(f\"Epoch {epoch+1}/{num_epochs} - Train: {train_loss:.4f}, Val: {val_loss:.4f}\")\n    \n    if val_loss < best_val_loss:\n        best_val_loss = val_loss\n        torch.save(model.state_dict(), 'best_model.pth')\n\nprint(f\"\\n✓ Training complete! Best val loss: {best_val_loss:.4f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Plot Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Train', linewidth=2, marker='o', markersize=4)\n",
    "plt.plot(val_losses, label='Val', linewidth=2, marker='s', markersize=4)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Progress', fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Improvement: {((train_losses[0]-train_losses[-1])/train_losses[0]*100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Test Enhancement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "model.load_state_dict(torch.load('best_model.pth'))\nmodel.eval()\n\ntest_noisy_file = noisy_files[0]\ntest_clean_file = clean_files[0]\n\nnoisy_audio, sr = librosa.load(test_noisy_file, sr=16000)\nclean_audio, sr = librosa.load(test_clean_file, sr=16000)\n\n# Enhance\nnoisy_stft = librosa.stft(noisy_audio, n_fft=512, hop_length=128)\nnoisy_mag = np.abs(noisy_stft)\nnoisy_phase = np.angle(noisy_stft)\n\n# Apply same normalization as training\nnoisy_mag_db = librosa.amplitude_to_db(noisy_mag, ref=np.max)\nnoisy_mag_norm = (noisy_mag_db + 80) / 80\nnoisy_mag_norm = np.clip(noisy_mag_norm, 0, 1)\n\ninput_tensor = torch.from_numpy(noisy_mag_norm).float().unsqueeze(0).unsqueeze(0).to(device)\n\nwith torch.no_grad():\n    enhanced_mag_norm = model(input_tensor).squeeze().cpu().numpy()\n\n# Denormalize: reverse the normalization\nenhanced_mag_db = (enhanced_mag_norm * 80) - 80\nenhanced_mag = librosa.db_to_amplitude(enhanced_mag_db)\n\n# Reconstruct with original phase\nenhanced_stft = enhanced_mag * np.exp(1j * noisy_phase)\nenhanced_audio = librosa.istft(enhanced_stft, hop_length=128, length=len(noisy_audio))\n\nprint(\"✓ Enhancement complete!\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11: Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_stft = librosa.stft(clean_audio, n_fft=512, hop_length=128)\n",
    "clean_mag = np.abs(clean_stft)\n",
    "\n",
    "fig, axes = plt.subplots(3, 2, figsize=(15, 10))\n",
    "\n",
    "axes[0,0].plot(noisy_audio[:sr], linewidth=0.5)\n",
    "axes[0,0].set_title('Noisy Waveform', fontweight='bold')\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "im1 = axes[0,1].imshow(librosa.amplitude_to_db(noisy_mag), aspect='auto', origin='lower', cmap='viridis')\n",
    "axes[0,1].set_title('Noisy Spectrogram', fontweight='bold')\n",
    "plt.colorbar(im1, ax=axes[0,1])\n",
    "\n",
    "axes[1,0].plot(clean_audio[:sr], linewidth=0.5)\n",
    "axes[1,0].set_title('Clean Waveform', fontweight='bold')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "im2 = axes[1,1].imshow(librosa.amplitude_to_db(clean_mag), aspect='auto', origin='lower', cmap='viridis')\n",
    "axes[1,1].set_title('Clean Spectrogram', fontweight='bold')\n",
    "plt.colorbar(im2, ax=axes[1,1])\n",
    "\n",
    "axes[2,0].plot(enhanced_audio[:sr], linewidth=0.5)\n",
    "axes[2,0].set_title('Enhanced Waveform', fontweight='bold')\n",
    "axes[2,0].grid(True, alpha=0.3)\n",
    "\n",
    "im3 = axes[2,1].imshow(librosa.amplitude_to_db(enhanced_mag), aspect='auto', origin='lower', cmap='viridis')\n",
    "axes[2,1].set_title('Enhanced Spectrogram', fontweight='bold')\n",
    "plt.colorbar(im3, ax=axes[2,1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12: Audio Playback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Noisy Audio:\")\n",
    "display(Audio(noisy_audio, rate=sr))\n",
    "\n",
    "print(\"\\nClean Audio:\")\n",
    "display(Audio(clean_audio, rate=sr))\n",
    "\n",
    "print(\"\\nEnhanced Audio:\")\n",
    "display(Audio(enhanced_audio, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13: Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_snr(clean, enhanced):\n",
    "    noise = enhanced - clean\n",
    "    signal_power = np.mean(clean ** 2)\n",
    "    noise_power = np.mean(noise ** 2)\n",
    "    return 10 * np.log10(signal_power / (noise_power + 1e-10))\n",
    "\n",
    "min_len = min(len(clean_audio), len(enhanced_audio), len(noisy_audio))\n",
    "clean_audio = clean_audio[:min_len]\n",
    "enhanced_audio = enhanced_audio[:min_len]\n",
    "noisy_audio = noisy_audio[:min_len]\n",
    "\n",
    "snr_noisy = compute_snr(clean_audio, noisy_audio)\n",
    "snr_enhanced = compute_snr(clean_audio, enhanced_audio)\n",
    "improvement = snr_enhanced - snr_noisy\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"SNR Noisy:      {snr_noisy:8.2f} dB\")\n",
    "print(f\"SNR Enhanced:   {snr_enhanced:8.2f} dB\")\n",
    "print(f\"Improvement:    {improvement:8.2f} dB\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n✓ Demo Complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}